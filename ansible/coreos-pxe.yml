# This playbook deploys a CoreOS Cluster via PXE/iPXE and Matchbox services
---
- hosts: all
  gather_facts: false
  tasks:
  - name: set coreos_linux_version
    set_fact:
      coreos_linux_version: "1745.7.0"
    run_once: true
    when: coreos_linux_version is undefined

  - name: set coreos_linux_channel
    set_fact:
      coreos_linux_channel: "stable"
    run_once: true
    when: coreos_linux_channel is undefined

- hosts: coreos_provisioner
  gather_facts: true
  become_user: admin
  remote_user: admin
  become: true
  vars:
    gen_script_folder: ../tmp/{{ cluster_pkg_folder }}
    deployment_scale: "{{ deployment_scale | default(50) }}"

  roles:
    - role: common
    - role: coreos-update-matchbox

- hosts: vmware_vms
  gather_facts: false
  vars:
    gen_script_folder: ../tmp/{{ cluster_pkg_folder }}
    deployment_scale: "{{ deployment_scale | default(50) }}"
  
  tasks:  

    - name: scan the VMS for the VM ID
      local_action:
        module: shell
        _raw_params: ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null {{ esxi_user }}@{{ esxi_host }} vim-cmd vmsvc/getallvms | grep " {{ inventory_hostname }} " | cut -d" " -f1
      register: vm_id
    
    - debug: msg="{{ vm_id.stdout }}"

    - name: find the MAC address
      local_action:
        module: shell
        _raw_params: ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null {{ esxi_user }}@{{ esxi_host }} vim-cmd vmsvc/device.getdevices {{ vm_id.stdout }} | grep macAddress | sed -e 's/         macAddress = "//g' | sed -e 's/", //g'
      register: vm_mac_addr
    
    - debug: msg="{{ vm_mac_addr.stdout_lines }}"

    - name: set MAC address fact
      set_fact:
        vm_mac_addr: "{{ vm_mac_addr.stdout }}"

    - name: stop the VMs
      local_action:
        module: shell
        _raw_params: ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null {{ esxi_user }}@{{ esxi_host }} vim-cmd vmsvc/power.off {{ vm_id.stdout }} 
      when: vm_id.stdout is defined
      ignore_errors: true

    - name: adjust the memSize value on the ESXi VM
      local_action:
        module: shell
        _raw_params: ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null {{ esxi_user }}@{{ esxi_host }} "sed '/memSize/s/.*/memSize\ =\ \"{{ memsize }}\"/' /vmfs/volumes/{{ esxi_ds }}/{{ inventory_hostname }}/{{ inventory_hostname }}.vmx > /vmfs/volumes/{{ esxi_ds }}/{{ inventory_hostname }}/{{ inventory_hostname }}.vmx-adjusted"
      when: memsize is defined

    - name: copy the adjusted vmx into place
      local_action:
        module: shell
        _raw_params: ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null {{ esxi_user }}@{{ esxi_host }} "cp /vmfs/volumes/{{ esxi_ds }}/{{ inventory_hostname }}/{{ inventory_hostname }}.vmx-adjusted /vmfs/volumes/{{ esxi_ds }}/{{ inventory_hostname }}/{{ inventory_hostname }}.vmx"
      when: numvcpus is defined

#    - name: Enable disk.EnableUUID the ESXi VM
#      local_action:
#        module: shell
#        _raw_params: ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null {{ esxi_user }}@{{ esxi_host }} "echo 'disk.EnableUUID = \"TRUE\"' >>  /vmfs/volumes/{{ esxi_ds }}/{{ inventory_hostname }}/{{ inventory_hostname }}.vmx-adjusted"
#      when: memsize is defined

#    - name: Copy the adjusted vmx into place
#      local_action:
#        module: shell
#        _raw_params: ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null {{ esxi_user }}@{{ esxi_host }} "cp /vmfs/volumes/{{ esxi_ds }}/{{ inventory_hostname }}/{{ inventory_hostname }}.vmx-adjusted /vmfs/volumes/{{ esxi_ds }}/{{ inventory_hostname }}/{{ inventory_hostname }}.vmx"
#      when: memsize is defined

    - name: adjust the numvcpus value on the ESXi VM
      local_action:
        module: shell
        _raw_params: ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null {{ esxi_user }}@{{ esxi_host }} "sed '/numvcpus/s/.*/numvcpus\ =\ \"{{ numvcpus }}\"/' /vmfs/volumes/{{ esxi_ds }}/{{ inventory_hostname }}/{{ inventory_hostname }}.vmx > /vmfs/volumes/{{ esxi_ds }}/{{ inventory_hostname }}/{{ inventory_hostname }}.vmx-adjusted"
      when: numvcpus is defined

    - name: copy the adjusted vmx into place
      local_action:
        module: shell
        _raw_params: ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null {{ esxi_user }}@{{ esxi_host }} "cp /vmfs/volumes/{{ esxi_ds }}/{{ inventory_hostname }}/{{ inventory_hostname }}.vmx-adjusted /vmfs/volumes/{{ esxi_ds }}/{{ inventory_hostname }}/{{ inventory_hostname }}.vmx"
      when: numvcpus is defined

    - name: adjust the Primary interface network name
      local_action:
        module: shell
        _raw_params: ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null {{ esxi_user }}@{{ esxi_host }} "sed '/ethernet0.networkName/s/.*/ethernet0.networkName\ =\ \"{{ esxi_net }}\"/' /vmfs/volumes/{{ esxi_ds }}/{{ inventory_hostname }}/{{ inventory_hostname }}.vmx > /vmfs/volumes/{{ esxi_ds }}/{{ inventory_hostname }}/{{ inventory_hostname }}.vmx-adjusted"
      when: esxi_net is defined

    - name: copy the adjusted vmx into place
      local_action:
        module: shell
        _raw_params: ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null {{ esxi_user }}@{{ esxi_host }} "cp /vmfs/volumes/{{ esxi_ds }}/{{ inventory_hostname }}/{{ inventory_hostname }}.vmx-adjusted /vmfs/volumes/{{ esxi_ds }}/{{ inventory_hostname }}/{{ inventory_hostname }}.vmx"
      when: esxi_net is defined

    - name: delete the adjusted vmx file
      local_action:
        module: shell
        _raw_params: ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null {{ esxi_user }}@{{ esxi_host }} "rm /vmfs/volumes/{{ esxi_ds }}/{{ inventory_hostname }}/{{ inventory_hostname }}.vmx-adjusted"
      ignore_errors: true


- hosts: coreos_controllers,coreos_workers
  remote_user: admin
  become: true
  any_errors_fatal: true
  gather_facts: false
  vars:

    tectonic_controller_domain: "{{ tectonic_controller_dn }}"

    tectonic_matchbox_http_url: "http://{{ provisioner_server_dn }}:8080"
    tectonic_matchbox_rpc_endpoint: "{{ provisioner_server_dn }}:8081"

    tectonic_ingress_domain: "{{ tectonic_ingress_dn }}"
    tectonic_container_linux_channel: "stable"
    tectonic_service_cidr: "10.3.0.0/16"
    tectonic_coreos_user: core
    tectonic_k8s_cluster_cidr: "10.42.0.0/16"

  tasks:

  - name: load tectonic_matchbox_ca
    local_action:
      module: shell
      _raw_params: "cat ../clusters/{{ cluster_pkg_folder }}/matchbox-certs/ca.crt"
    register: tectonic_matchbox_ca_load
    become: false
    run_once: true

  - debug: msg="{{ tectonic_matchbox_ca_load.stdout }}" 
    run_once: true

  - name: set tectonic_matchbox_ca
    set_fact:
      tectonic_matchbox_ca: "{{ tectonic_matchbox_ca_load.stdout }}"
    run_once: true

  - name: set k8s_cluster_cidr
    set_fact:
      k8s_cluster_cidr: "{{ tectonic_k8s_cluster_cidr }}"
    run_once: true
    when: k8s_cluster_cidr is undefined

  - name: set vanilla k8s default to false (Full Tectonic Install)
    set_fact:
      tectonic_vanilla_k8s: false
    run_once: true
    when: tectonic_vanilla_k8s is undefined

  - name: load tectonic_matchbox_client_cert
    local_action:
      module: shell
      _raw_params: "cat ../clusters/{{ cluster_pkg_folder }}/matchbox-certs/client.crt"
    register: tectonic_matchbox_client_cert_load
    become: false
    run_once: true

  - debug: msg="{{ tectonic_matchbox_client_cert_load.stdout }}" 
    run_once: true

  - name: set tectonic_matchbox_client_cert
    set_fact:
      tectonic_matchbox_client_cert: "{{ tectonic_matchbox_client_cert_load.stdout }}"
    run_once: true

  - name: load tectonic_matchbox_client_key
    local_action:
      module: shell
      _raw_params: "cat ../clusters/{{ cluster_pkg_folder }}/matchbox-certs/client.key"
    register: tectonic_matchbox_client_key_load
    become: false
    run_once: true

  - debug: msg="{{ tectonic_matchbox_client_key_load.stdout }}" 
    run_once: true

  - name: set tectonic_matchbox_client_key
    set_fact:
      tectonic_matchbox_client_key: "{{ tectonic_matchbox_client_key_load.stdout }}"
    run_once: true

  - name: load ssh key
    local_action:
      module: shell
      _raw_params: "cat ../node-packer/keys/authorized_keys"
    register: tectonic_ssh_key_load
    when: tectonic_ssh_authorized_key is undefined
    become: false
    run_once: true

  - debug: msg="{{  tectonic_ssh_key_load.stdout }}" 
    run_once: true

  - name: set tectonic_ssh_authorized_key
    set_fact:
      tectonic_ssh_authorized_key: "{{  tectonic_ssh_key_load.stdout }}"
    run_once: true
    when: tectonic_ssh_authorized_key is undefined

  - name: generate the terraform.vars file
    local_action:
      module: template
      src: templates/coreos-terraform.j2
      dest: ../clusters/{{ cluster_pkg_folder }}/terraform.tfvars
      mode: 0766
    become: false
    run_once: true

  - name: generate custom iSCSI kublet service def
    local_action:
      module: template
      src: templates/coreos-kubelet-service-iscsi.j2
      dest: ../clusters/{{ cluster_pkg_folder }}/coreos-kublet-service
      mode: 0766
    become: false
    when: tectonic_networking != "canal"

  - name: generate custom iSCSI/Canal kublet service def
    local_action:
      module: template
      src: templates/coreos-kubelet-service-iscsi-canal.j2
      dest: ../clusters/{{ cluster_pkg_folder }}/coreos-kublet-service
      mode: 0766
    become: false
    when: tectonic_networking == "canal"

  - name: generate the iSCSI script
    local_action:
      module: template
      src: templates/coreos-iscsi-script.j2
      dest: ../clusters/{{ cluster_pkg_folder }}/coreos-iscsi.sh
      mode: 0766
    become: false

  - name: generate the masters.csv file
    local_action:
      module: template
      src: templates/coreos-masters-csv.j2
      dest: ../clusters/{{ cluster_pkg_folder }}/manual-masters.csv
      mode: 0766
    become: false
    run_once: true

  - name: generate the workers.csv file
    local_action:
      module: template
      src: templates/coreos-workers-csv.j2
      dest: ../clusters/{{ cluster_pkg_folder }}/manual-workers.csv
      mode: 0766
    become: false
    run_once: true

- hosts: coreos_provisioner,coreos_workers,coreos_controllers
  remote_user: admin
  become: true
  any_errors_fatal: true
  gather_facts: false
  vars:

  tasks:

  - name: process the DHCP template file
    local_action:
      module: template
      src: templates/coreos-dhcpd-static-block.j2
      dest: ../tmp/coreos-dhcpd-block.txt
    become: false
    when: inventory_hostname in groups['coreos_provisioner']
    run_once: true

  - name: load dhcpd block
    local_action:
      module: shell
      _raw_params: "cat ../tmp/coreos-dhcpd-block.txt"
    register: coreos_dhcpd_static_block
    become: false
    run_once: true

  - debug: msg="{{ coreos_dhcpd_static_block.stdout_lines }}" 
    run_once: true

  - name: stop the DHCPD server
    service:
      name: dhcpd
      state: stopped
    when: inventory_hostname in groups['coreos_provisioner']

  - name: remove the DHCP server with the static blocks
    blockinfile:
      dest: /etc/dhcp/dhcpd.conf
      state: absent
      marker: "# {mark} {{ cluster_name }} CLUSTER MANAGED BLOCK"
      block: |
        {{ coreos_dhcpd_static_block.stdout }}
    when: inventory_hostname in groups['coreos_provisioner']

  - name: update the DHCP server with the static blocks
    blockinfile:
      dest: /etc/dhcp/dhcpd.conf
      insertafter: "EOF"
      marker: "# {mark} {{ cluster_name }} CLUSTER MANAGED BLOCK"
      block: |
        {{ coreos_dhcpd_static_block.stdout }}
    when: inventory_hostname in groups['coreos_provisioner']

  - name: start the DHCPD server
    service:
      name: dhcpd
      state: started
    when: inventory_hostname in groups['coreos_provisioner']
        
  - name: clean up tmp dhcpd block
    local_action:
      module: shell
      _raw_params: "rm ../tmp/coreos-dhcpd-block.txt"
    become: false
    run_once: true
        